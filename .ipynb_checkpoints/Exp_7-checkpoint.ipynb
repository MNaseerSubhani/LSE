{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQrj7wcIQqHG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from options.train_options import TrainOptions\n",
    "from options.test_options import TestOptions\n",
    "import os\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from data import CreateSrcDataLoader\n",
    "from data import CreateTrgDataLoader\n",
    "from model import CreateModel\n",
    "# from model import CreateDiscriminator\n",
    "from utils.timer import Timer\n",
    "import tensorboardX\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from IPython.display import clear_output\n",
    "# import evaluation\n",
    "import os.path as osp\n",
    "import collections\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import time\n",
    "import run_time_evaluation\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSLRr4KNQqHN"
   },
   "source": [
    "\n",
    "Init settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_LqItd0QqHO"
   },
   "outputs": [],
   "source": [
    "root_base = '/home/olektra_gpu/Desktop/olektra projects/ECCV'                   #Root directory\n",
    "sorted_list = root_base +'/dataset/cityscapes_list/sorted.txt'         # Final sorted list's path\n",
    "data_gen_list = root_base +'/dataset/cityscapes_list/data_gen_list.txt'# data generated list's path\n",
    "generated_data = root_base +'/dataset/generated_data/'                 # generated data path\n",
    "generated_data_path = root_base +\"/dataset/generated_data/\"\n",
    "\n",
    "\n",
    "select_model = 'DeepLab'                                                   # 'VGG' or 'DeepLab'\n",
    "source_data = 'gta5'                                                   # 'synthia' or 'gta5'\n",
    "model_pick = 'init'                                                    #'init' or 'annex'\n",
    "model_weights = 'gta5_DeepLab__init'                                 # weights file's name\n",
    "\n",
    "\n",
    "IMG_W = 1024                                                           #Image width\n",
    "IMG_H = 512                                                            #Image height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2h7MsZNQqHT"
   },
   "outputs": [],
   "source": [
    "eval_list = root_base +'/dataset/cityscapes_list/eval_.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJz5H6Y8QqHX"
   },
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oD233fo3QqHZ"
   },
   "outputs": [],
   "source": [
    "file_name_save = root_base + '/snapshots/mIoU.txt'\n",
    "def print_args(args):\n",
    "    message = ''\n",
    "    message += '----------------- Options ---------------\\n'\n",
    "    for k, v in sorted(vars(args).items()):\n",
    "        comment = ''\n",
    "        message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n",
    "    message += '--------------------------------------'\n",
    "   \n",
    "\n",
    "    # save to the disk\n",
    "    \n",
    "    with open(file_name_save, 'a') as args_file:\n",
    "        args_file.write(message)\n",
    "        args_file.write('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BMOirKFQqHd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_args(args):\n",
    "    args.model =select_model\n",
    "    args.source=source_data\n",
    "    args.data_dir=root_base + '/dataset/'+args.source\n",
    "    args.data_list = root_base+'/dataset/'+args.source+'_list/train.txt'\n",
    "    args.data_list_target = root_base +'/dataset/cityscapes_list/train.txt'\n",
    "    args.restore_from   = root_base + '/init_models/' + model_pick+ '/' + model_weights   #init_weights\n",
    "    args.batch_size = 1\n",
    "    args.weight_decay = 0.0005\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX_YDSVSQqHj"
   },
   "outputs": [],
   "source": [
    "classes = ['road' , 'side walk', 'building' , 'wall', 'fence', 'pole', 'trafic lights', 'trafic sign', 'vegitation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck','bus', 'train', 'motorcycle', 'bicycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGqfuCH7QqHn"
   },
   "outputs": [],
   "source": [
    "def keras_out(inp):\n",
    "    inp = np.rollaxis(inp,axis =-1)\n",
    "    inp = np.rollaxis(inp,axis =-1)\n",
    "    inp = inp.reshape(1,inp.shape[0],inp.shape[1],inp.shape[2])\n",
    "    \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9L_pUw6QqHr"
   },
   "outputs": [],
   "source": [
    "def Plot_img(out):\n",
    "    plt.imshow(out.reshape(IMG_H,IMG_W,3),interpolation='nearest')\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iChmTptQqHv"
   },
   "outputs": [],
   "source": [
    "def self_entropy(pred, epsilon=1e-12):\n",
    "    pred = pred[0]\n",
    "    p = pred * np.log(pred+ epsilon)\n",
    "    map_ = -np.sum(p, -1)\n",
    "    \n",
    "    return map_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6rRzgu2QqH0"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Load areas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3253,
     "status": "ok",
     "timestamp": 1581505345641,
     "user": {
      "displayName": "Muhammad Naseer Subhani",
      "photoUrl": "",
      "userId": "06699857406068047110"
     },
     "user_tz": -300
    },
    "id": "orupHvbzQqH2",
    "outputId": "bb82a0a0-159d-4b3f-fe7c-eace551f845f"
   },
   "outputs": [],
   "source": [
    "# area_save_path = root_base + \"/areas_and_spatial_priors/area_classes_s.npy\"\n",
    "# class_mArea_man = np.load(area_save_path)\n",
    "# arr=np.ones((19))\n",
    "# arr[0:19] = class_mArea_man\n",
    "# weights =  (arr)\n",
    "# weight_mat = np.zeros((512,1024,19))\n",
    "# for i in range(512):\n",
    "#     for j in range(1024):\n",
    "#         weight_mat[i][j] = weights\n",
    "        \n",
    "# weight_mat = np.rollaxis(weight_mat,axis = -1)\n",
    "# print(weight_mat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3162,
     "status": "ok",
     "timestamp": 1581505345643,
     "user": {
      "displayName": "Muhammad Naseer Subhani",
      "photoUrl": "",
      "userId": "06699857406068047110"
     },
     "user_tz": -300
    },
    "id": "aKryf5cNQqH5",
    "outputId": "ae2e653b-4cf8-4353-b128-878c7ccc0471"
   },
   "outputs": [],
   "source": [
    "# weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YN0fpZBaQqH-"
   },
   "source": [
    "Load priors of source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhWQMiuoQqH_"
   },
   "outputs": [],
   "source": [
    "# save_pth = root_base + \"/priors_of_source/priors_gta5.npy\"\n",
    "# priors_ = np.load(save_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2oexeRvQqIC"
   },
   "outputs": [],
   "source": [
    "# priors__ = resize(priors_, (19,1024, 2048), anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJSZ910PQqIG"
   },
   "outputs": [],
   "source": [
    "# from scipy.ndimage.filters import gaussian_filter\n",
    "# source_priros= np.zeros((19,1024,2048))\n",
    "# for j in range(19):\n",
    "    \n",
    "#     source_priros[j] = gaussian_filter(priors__[j], sigma=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzxLGvvWQqIJ"
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (5,4))\n",
    "# columns = 5\n",
    "# rows = 4\n",
    "\n",
    "# for i in range(1, columns * rows ):\n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     fig.set_size_inches(18.5,10.5)\n",
    "#     plt.imshow(source_priros[i-1],interpolation = 'nearest')\n",
    "#     plt.grid(False)\n",
    "#     plt.title(classes[i-1])\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2RNiWGQQqIN"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTE_nj2iQqIN"
   },
   "outputs": [],
   "source": [
    "def ent_normalization(SE,Y_pre):\n",
    "    tY_pre = Y_pre[0,:,:,:]\n",
    "    labs = np.argmax(tY_pre, axis=-1)\n",
    "    labs1 = labs.flatten()\n",
    "    se1 = SE.flatten()\n",
    "\n",
    "    uniq = np.unique(labs1)\n",
    "\n",
    "    for un in range(uniq.shape[0]):\n",
    "\n",
    "        t_se = se1[labs1==uniq[un]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        t_se1 = (t_se - t_se.min() )/(t_se.max()-t_se.min() )\n",
    "\n",
    "        se1[labs1==uniq[un]] = t_se1\n",
    "\n",
    "    SE = se1.reshape(SE.shape)\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAerP0MOQqIQ"
   },
   "outputs": [],
   "source": [
    "class model_init():\n",
    "    def __init__(self):\n",
    "        opt = TrainOptions()\n",
    "        args = opt.initialize()\n",
    "        \n",
    "        \n",
    "        change_args(args)\n",
    "        _t = {'iter time' : Timer()}\n",
    "        model_name = args.source + '_to_' + args.target\n",
    "\n",
    "        if not os.path.exists(args.snapshot_dir):\n",
    "            os.makedirs(args.snapshot_dir)   \n",
    "            os.makedirs(os.path.join(args.snapshot_dir, 'logs'))\n",
    "  \n",
    "        \n",
    "        \n",
    "        model, optimizer = CreateModel(args)\n",
    " \n",
    "\n",
    "        self.args =args\n",
    "        self.model_name =model_name\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.opt = opt\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_mcOzbAQqIU"
   },
   "outputs": [],
   "source": [
    "IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0brE1hDQqIY"
   },
   "outputs": [],
   "source": [
    "def load_single_image(name):\n",
    "    \n",
    "    image = Image.open(osp.join(root_base + '/dataset/cityscapes/', \"leftImg8bit/%s/%s\" % ('train', name))).convert('RGB')\n",
    "    # resize\n",
    "    image = image.resize((IMG_W,IMG_H), Image.BICUBIC)\n",
    "\n",
    "    rl_image = np.asarray(image, np.float32)\n",
    "    rl_image_rgb = rl_image.copy()\n",
    "\n",
    "\n",
    "    size = rl_image.shape\n",
    "    image = rl_image[:, :, ::-1]  # change to BGR\n",
    "    image -= IMG_MEAN\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    \n",
    "\n",
    "    return image.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3PXeESIQqIb"
   },
   "outputs": [],
   "source": [
    "\n",
    "epsilon = 1e-12\n",
    "cnt_img =0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Base_Adaptation():\n",
    "    def __init__(self,model_loader):\n",
    "        self.args =model_loader.args\n",
    "        self.model_name =model_loader.model_name\n",
    "        self.model = model_loader.model\n",
    "        self.optimizer = model_loader.optimizer\n",
    "        self.cnt_img = 0\n",
    "        self.entropy_th_class = np.ones((1,19)) #* self.args.entropy_th\n",
    "        \n",
    "    def sorting(self):\n",
    "        print(\"*********** Finding most confident samples using\",self.args.sorting_method,\" ***********\")\n",
    "        if(self.args.sorting_method == 'class_wise'):\n",
    "            self.class_wise_sort()\n",
    "        else:\n",
    "            self.global_sort()\n",
    "        print(\"***************END********************\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    def class_wise_sort(self):\n",
    "        self.args.data_label_folder_target = None \n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = root_base +'/dataset/cityscapes_list/train.txt'\n",
    "        self.args.num_steps = self.args.total_no_of_target\n",
    "        self.args.batch_size = 1\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        data_save_class = {classes[0]:[],classes[1]:[],classes[2]:[],classes[3]:[],classes[4]:[],classes[5]:[],classes[6]:[],classes[7]:[],classes[8]:[],classes[9]:[],classes[10]:[],classes[11]:[],classes[12]:[],classes[13]:[],classes[14]:[],classes[15]:[],classes[16]:[],classes[17]:[],classes[18]:[],\"f_name_\"+classes[0]:[],\"f_name_\"+classes[1]:[],\"f_name_\"+classes[2]:[],\"f_name_\"+classes[3]:[],\"f_name_\"+classes[4]:[],\"f_name_\"+classes[5]:[],\"f_name_\"+classes[6]:[],\"f_name_\"+classes[7]:[],\"f_name_\"+classes[8]:[],\"f_name_\"+classes[9]:[],\"f_name_\"+classes[10]:[],\"f_name_\"+classes[11]:[],\"f_name_\"+classes[12]:[],\"f_name_\"+classes[13]:[],\"f_name_\"+classes[14]:[],\"f_name_\"+classes[15]:[],\"f_name_\"+classes[16]:[],\"f_name_\"+classes[17]:[],\"f_name_\"+classes[18]:[]}\n",
    "        data_save_class_sorted = {classes[0]:[],classes[1]:[],classes[2]:[],classes[3]:[],classes[4]:[],classes[5]:[],classes[6]:[],classes[7]:[],classes[8]:[],classes[9]:[],classes[10]:[],classes[11]:[],classes[12]:[],classes[13]:[],classes[14]:[],classes[15]:[],classes[16]:[],classes[17]:[],classes[18]:[],\"f_name_\"+classes[0]:[],\"f_name_\"+classes[1]:[],\"f_name_\"+classes[2]:[],\"f_name_\"+classes[3]:[],\"f_name_\"+classes[4]:[],\"f_name_\"+classes[5]:[],\"f_name_\"+classes[6]:[],\"f_name_\"+classes[7]:[],\"f_name_\"+classes[8]:[],\"f_name_\"+classes[9]:[],\"f_name_\"+classes[10]:[],\"f_name_\"+classes[11]:[],\"f_name_\"+classes[12]:[],\"f_name_\"+classes[13]:[],\"f_name_\"+classes[14]:[],\"f_name_\"+classes[15]:[],\"f_name_\"+classes[16]:[],\"f_name_\"+classes[17]:[],\"f_name_\"+classes[18]:[]}\n",
    "    \n",
    "        \n",
    "        \n",
    "        for index, batch in tqdm.tqdm(enumerate(targetloader)):\n",
    "            \n",
    "            image, _, name ,__= batch\n",
    "            fn = name[0]\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            y_pred = keras_out(output)\n",
    "            \n",
    "            \n",
    "            \n",
    "            MP_max = np.max(y_pred,axis = -1)[0]\n",
    "            LP_argmax = np.argmax(y_pred,axis = -1)[0]\n",
    "            \n",
    "      \n",
    "            for c in range(len(classes) ):\n",
    "                M_c = MP_max[LP_argmax == c]\n",
    "                if(M_c.any()):\n",
    "                    data_save_class['f_name_'+classes[c]].append(fn)\n",
    "                    data_save_class[classes[c]].append(np.mean(M_c))\n",
    "        \n",
    "\n",
    "        f = open(sorted_list,\"w+\") \n",
    "        f_ev = open(eval_list,\"w+\") \n",
    "        self.cnt_img=0\n",
    "        over_all_list = []\n",
    "        for c in range(len(classes) ):      \n",
    "            \n",
    "            if(source_data == 'synthia'):\n",
    "                if(c !=9 and c !=14 and c !=16):\n",
    "                    max_prob_sorted_ascending, f_name_arrays = zip(*sorted(zip(data_save_class[classes[c]], data_save_class['f_name_'+classes[c]])))\n",
    "                    f_name_arrays = f_name_arrays[::-1]\n",
    "                    max_prob_sorted_ascending = max_prob_sorted_ascending[::-1]\n",
    "\n",
    "                    data_save_class_sorted[classes[c]] = max_prob_sorted_ascending\n",
    "                    data_save_class_sorted['f_name_'+classes[c]] = f_name_arrays\n",
    "                    select_imgs =  int(len(f_name_arrays) * (self.args.p/19))\n",
    "\n",
    "                    selct = f_name_arrays[0:select_imgs]\n",
    "\n",
    "\n",
    "\n",
    "                    ######Dynamic threshold of each class##############\n",
    "\n",
    "                    f_n = f_name_arrays[select_imgs]\n",
    "                    img= load_single_image(f_n)\n",
    "                    img = img.reshape(1,3,IMG_H,IMG_W)\n",
    "                    img = torch.from_numpy(img).float().to(device)\n",
    "                    out_ = self.model(Variable(img).cuda())\n",
    "                    out_ = nn.functional.softmax(out_, dim=1)\n",
    "                    out_=out_.cpu().data[0].numpy()\n",
    "                    y_pred = keras_out(out_)\n",
    "\n",
    "\n",
    "                    Y_argmax = np.argmax(y_pred,axis = -1)[0]\n",
    "                    SE_main = self_entropy(y_pred)\n",
    "\n",
    "                    if(self.args.entropy_normalization):\n",
    "                        SE_main = ent_normalization(SE_main,y_pred)\n",
    "\n",
    "                    class_se = SE_main[Y_argmax == c]\n",
    "                    mean_class_se = np.mean(class_se) * self.args.ent_th_on_class_gain\n",
    "\n",
    "\n",
    "                    self.entropy_th_class[0][c] = mean_class_se\n",
    "\n",
    "                    ##############################################\n",
    "\n",
    "                    over_all_list += selct\n",
    "            else:\n",
    "                max_prob_sorted_ascending, f_name_arrays = zip(*sorted(zip(data_save_class[classes[c]], data_save_class['f_name_'+classes[c]])))\n",
    "                f_name_arrays = f_name_arrays[::-1]\n",
    "                max_prob_sorted_ascending = max_prob_sorted_ascending[::-1]\n",
    "\n",
    "                data_save_class_sorted[classes[c]] = max_prob_sorted_ascending\n",
    "                data_save_class_sorted['f_name_'+classes[c]] = f_name_arrays\n",
    "                select_imgs =  int(len(f_name_arrays) * (self.args.p/19))\n",
    "\n",
    "                selct = f_name_arrays[0:select_imgs]\n",
    "                f_ev.write(classes[c]+'\\n')\n",
    "                for loop in f_name_arrays[0:10]:\n",
    "                    f_ev.write(loop+'\\n')\n",
    "                \n",
    "                    \n",
    "                \n",
    "\n",
    "                ######Dynamic threshold of each class##############\n",
    "\n",
    "                f_n = f_name_arrays[select_imgs]\n",
    "                img= load_single_image(f_n)\n",
    "                img = img.reshape(1,3,IMG_H,IMG_W)\n",
    "                img = torch.from_numpy(img).float().to(device)\n",
    "                out_ = self.model(Variable(img).cuda())\n",
    "                out_ = nn.functional.softmax(out_, dim=1)\n",
    "                out_=out_.cpu().data[0].numpy()\n",
    "                y_pred = keras_out(out_)\n",
    "\n",
    "\n",
    "                Y_argmax = np.argmax(y_pred,axis = -1)[0]\n",
    "                SE_main = self_entropy(y_pred)\n",
    "\n",
    "                if(self.args.entropy_normalization):\n",
    "                    SE_main = ent_normalization(SE_main,y_pred)\n",
    "\n",
    "                class_se = SE_main[Y_argmax == c]\n",
    "                mean_class_se = np.mean(class_se) * self.args.ent_th_on_class_gain\n",
    "\n",
    "\n",
    "                self.entropy_th_class[0][c] = mean_class_se\n",
    "\n",
    "                ##############################################\n",
    "\n",
    "                over_all_list += selct\n",
    "            \n",
    "        over_all_list = list(dict.fromkeys(over_all_list))\n",
    "        for loop in over_all_list:\n",
    "            f.write(loop+'\\n')\n",
    "            self.cnt_img +=1\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def global_sort(self):\n",
    "        self.args.data_label_folder_target = None \n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = root_base +'/dataset/cityscapes_list/train.txt'\n",
    "        self.args.num_steps = self.args.total_no_of_target\n",
    "        self.args.batch_size = 1\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        data_save_global = {\"f_name\":[],\"max_prob_values\":[]} \n",
    "        \n",
    "        for index, batch in tqdm.tqdm(enumerate(targetloader)):\n",
    "            \n",
    "            image, _, name ,__= batch\n",
    "            fn = name[0]\n",
    "#             print(fn)\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            y_pred = keras_out(output)\n",
    "\n",
    "            MP_max = np.max(y_pred,axis = -1)[0]\n",
    "          \n",
    "            data_save_global['f_name'].append(fn)\n",
    "            data_save_global['max_prob_values'].append(np.mean(MP_max))\n",
    "\n",
    "        max_prob_sorted_ascending, f_name_arrays = zip(*sorted(zip(data_save_global['max_prob_values'], data_save_global['f_name'])))\n",
    "        f_name_arrays = f_name_arrays[::-1]\n",
    "        max_prob_sorted_ascending = max_prob_sorted_ascending[::-1]\n",
    "\n",
    "        data_save_global['f_name'] = f_name_arrays\n",
    "        data_save_global['max_prob_values'] = max_prob_sorted_ascending\n",
    "        \n",
    "        f = open(sorted_list,\"w+\")\n",
    "        for loop in data_save_global['f_name']:\n",
    "            f.write(loop+'\\n')\n",
    "            \n",
    "        \n",
    "          \n",
    "    \n",
    "    def save_data(self,patch_x, patch_y, path,i,map_):\n",
    "      \n",
    "        Y = np.argmax(patch_y,axis = -1)\n",
    "        if(map_ != 'a'):\n",
    "            Y[map_ == 0] = 255\n",
    "        plt.imsave( path +\"images/\"+ str(i)+'.png',patch_x)  #p_small_data\n",
    "        Y=np.asarray(Y,dtype=np.uint8)\n",
    "        \n",
    "       \n",
    "        Y=Image.fromarray(Y,mode = 'L')\n",
    "\n",
    "        \n",
    "        Y.save(path+ \"labels/\"+str(i)+'.png')\n",
    "        \n",
    "    def Data_Generate(self):\n",
    "        if(self.args.patch_select_method == 'smart'):\n",
    "            self.data_generate_smart()\n",
    "        else:\n",
    "            self.data_generator()\n",
    "            \n",
    "    def data_generate_smart(self):\n",
    "        self.args.data_label_folder_target = None\n",
    "        if(self.args.sorting_method == 'class_wise'):\n",
    "            stop_point = self.cnt_img\n",
    "        else:\n",
    "            stop_point = int(self.args.p * self.args.total_no_of_target)\n",
    "            \n",
    "       \n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        \n",
    "        self.args.batch_size = 1\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        f = open(data_gen_list,\"w+\")\n",
    "        print(\"***********Generating Data smart***********\")\n",
    "        print(\"Total it : \",stop_point+1)\n",
    "        for index, batch in tqdm.tqdm(enumerate(targetloader)):\n",
    "            \n",
    "            if(index > stop_point):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            fn = name[0]\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            \n",
    "            image=image.cpu().data[0].numpy()\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            \n",
    "            if(self.args.balance_on_priors):\n",
    "                wei = (1 - weight_mat )\n",
    "                output = np.multiply(output,wei)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            ################################################\n",
    "            map_ = np.zeros((IMG_H,IMG_W))\n",
    "            SE = self_entropy(Y_pred)\n",
    "            \n",
    "            if(self.args.entropy_normalization):\n",
    "                SE = ent_normalization(SE,Y_pred)\n",
    "            \n",
    "            if(self.args.ent_th_on_class):\n",
    "                \n",
    "                for il in range(IMG_H):\n",
    "                    for ij in range(IMG_W):\n",
    "                        if(SE[il][ij] >= self.entropy_th_class[0][np.argmax(Y_pred[0][il][ij],axis = -1)]):\n",
    "                            map_[il][ij] =  0\n",
    "                        else:\n",
    "                            map_[il][ij] =  1\n",
    "                                    \n",
    "            else:\n",
    "            \n",
    "                map_[SE >= self.args.entropy_th] =  0\n",
    "                map_[SE < self.args.entropy_th] =  1\n",
    "            cnt_flag = 0\n",
    "            \n",
    "            ###################save orignal#################\n",
    "            \n",
    "#             f.write(str(count)+\".png\"+'\\n')\n",
    "#             self.save_data(rl_img, Y_pred.reshape(IMG_H,IMG_W,19), generated_data,(count),'a')\n",
    "#             count +=1\n",
    "            ##########################################\n",
    "            \n",
    "            \n",
    "            for patch in range(self.args.total_patches_in_smart):\n",
    "\n",
    "                point_X_s = self.args.patch_size[1]\n",
    "                point_Y_s = self.args.patch_size[0]\n",
    "\n",
    "                \n",
    "                point_X = random.randint(0, IMG_W-point_X_s)\n",
    "                point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "                \n",
    "                #########################################\n",
    "                x_mean_patch =  image[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "                map_patch = map_[point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s)\n",
    "   \n",
    "                x_PATCH =  rl_img[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "                y_PATCH =  Y_pred[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "                \n",
    "            \n",
    "                y_PATCH = np.rollaxis(y_PATCH[0], axis = -1).reshape(1,19,point_Y_s,point_X_s)\n",
    "                y_PATCH = torch.from_numpy(y_PATCH).float().to(device)\n",
    "                Y_Patch_resize = nn.functional.upsample(y_PATCH, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1)\n",
    "                Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1).reshape(1,IMG_H,IMG_W,19)\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "                #############smart patch seperateeeeeer#########\n",
    "                x_mean_patch = np.rollaxis(x_mean_patch, axis = -1).reshape(1,3,point_Y_s,point_X_s)\n",
    "                x_mean_patch = torch.from_numpy(x_mean_patch).float().to(device)\n",
    "                x_mean_res = nn.functional.upsample(x_mean_patch, (IMG_H, IMG_W), mode='bilinear', align_corners=True)\n",
    "                \n",
    "                \n",
    "                output = self.model(Variable(x_mean_res).cuda())\n",
    "                output = nn.functional.softmax(output, dim=1)\n",
    "                output=output.cpu().data[0].numpy()\n",
    "                y_patch_out = keras_out(output)\n",
    "                \n",
    "                SE_main = self_entropy(Y_Patch_resize)\n",
    "                SE_patch = self_entropy(y_patch_out)\n",
    "                \n",
    "                diff = (np.mean(SE_patch) - np.mean(SE_main))\n",
    "                \n",
    "                ##################################\n",
    "                \n",
    "                if(diff >= self.args.entropy_diff_th):\n",
    "                    #####################code new ################\n",
    "                    x_PATCH = np.rollaxis(x_PATCH, axis = -1).reshape(1,3,point_Y_s,point_X_s)\n",
    "                    x_PATCH = torch.from_numpy(x_PATCH).float().to(device)\n",
    "\n",
    "                    \n",
    "                    map_patch = map_patch.reshape(1,1,point_Y_s,point_X_s)\n",
    "                    map_patch = torch.from_numpy(map_patch).float().to(device)\n",
    "\n",
    "                    X_Patch_resize = nn.functional.upsample(x_PATCH, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    X_Patch_resize = np.rollaxis(X_Patch_resize, axis = -1)\n",
    "                    X_Patch_resize = np.rollaxis(X_Patch_resize, axis = -1)\n",
    "\n",
    "\n",
    "                    Y_Patch_resize = nn.functional.upsample(y_PATCH, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1)\n",
    "                    Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1).reshape(1,IMG_H,IMG_W,19)\n",
    "\n",
    "\n",
    "                    map_patch_re = nn.functional.upsample(map_patch, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    map_patch_re = map_patch_re[0]\n",
    "\n",
    "                    #############################################\n",
    "                    cnt_flag +=1\n",
    "                    f.write(str(count+cnt_flag)+\".png\"+'\\n')\n",
    "                    self.save_data(X_Patch_resize, Y_Patch_resize.reshape(IMG_H,IMG_W,19), generated_data,(count+cnt_flag),map_patch_re)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                if(cnt_flag >= self.args.no_of_patches_per_image):\n",
    "                    break\n",
    "            count+=self.args.no_of_patches_per_image + 1\n",
    "            \n",
    "        print(\"*****************END****************\")\n",
    "        \n",
    "        \n",
    "    def data_generator(self):\n",
    "        self.args.data_label_folder_target = None\n",
    "        if(self.args.sorting_method == 'class_wise'):\n",
    "            stop_point = self.cnt_img\n",
    "        else:\n",
    "            stop_point = int(self.args.p * self.args.total_no_of_target)\n",
    "            \n",
    "       \n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        \n",
    "        self.args.batch_size = 1\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        f = open(data_gen_list,\"w+\")\n",
    "        print(\"***********Generating Data Random***********\")\n",
    "        print(\"Total it : \",stop_point+1)\n",
    "        for index, batch in tqdm.tqdm(enumerate(targetloader)):\n",
    "            \n",
    "            if(index > stop_point):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            fn = name[0]\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "             \n",
    "            \n",
    "            if(self.args.balance_on_priors):\n",
    "                wei = (1 - weight_mat )\n",
    "                output = np.multiply(output,wei)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            ###############################################\n",
    "            map_ = np.zeros((IMG_H,IMG_W))\n",
    "            SE = self_entropy(Y_pred)\n",
    "            \n",
    "            if(self.args.entropy_normalization):\n",
    "                SE = ent_normalization(SE,Y_pred)\n",
    "            \n",
    "            if(self.args.ent_th_on_class):\n",
    "                \n",
    "                for il in range(IMG_H):\n",
    "                    for ij in range(IMG_W):\n",
    "                        \n",
    "                        if(SE[il][ij] >= self.entropy_th_class[0][np.argmax(Y_pred[0][il][ij],axis = -1)]):\n",
    "                            map_[il][ij] =  0\n",
    "                        else:\n",
    "                            map_[il][ij] =  1\n",
    "                                    \n",
    "            else:\n",
    "            \n",
    "                map_[SE >= self.args.entropy_th] =  0\n",
    "                map_[SE < self.args.entropy_th] =  1\n",
    "            ###################save orignal#################\n",
    "            \n",
    "#             f.write(str(count)+\".png\"+'\\n')\n",
    "#             self.save_data(rl_img, Y_pred.reshape(IMG_H,IMG_W,19), generated_data,(count),map_)\n",
    "#             count +=1\n",
    "            ##########################################\n",
    "            \n",
    "            if(self.args.shrink_image):\n",
    "                f.write(str(count)+\".png\"+'\\n')\n",
    "                IMG_SS = np.zeros((IMG_H,IMG_W,3))\n",
    "                Y_PRE_SS = np.zeros((1,IMG_H,IMG_W,19))\n",
    "                MAP_SS= np.zeros((IMG_H,IMG_W))\n",
    "                \n",
    "                img_ss = resize(rl_img, (self.args.patch_size[0], self.args.patch_size[1]), anti_aliasing=True)\n",
    "                y_pred_ss = resize(Y_pred, (1,self.args.patch_size[0], self.args.patch_size[1]), anti_aliasing=False)\n",
    "                \n",
    "                point_X_s = self.args.patch_size[1]\n",
    "                point_Y_s = self.args.patch_size[0]\n",
    "\n",
    "\n",
    "#                 point_X = random.randint(0, IMG_W-point_X_s)\n",
    "#                 point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "                \n",
    "                point_X = 256\n",
    "                point_Y = 128\n",
    "                \n",
    "\n",
    "                IMG_SS[point_Y:point_Y+self.args.patch_size[0] , point_X:point_X+self.args.patch_size[1]]= img_ss\n",
    "                Y_PRE_SS[0,point_Y:point_Y+self.args.patch_size[0] , point_X:point_X+self.args.patch_size[1]]= y_pred_ss\n",
    "                MAP_SS[point_Y:point_Y+self.args.patch_size[0] , point_X:point_X+self.args.patch_size[1]] =1\n",
    "                \n",
    "                self.save_data(IMG_SS, Y_PRE_SS.reshape(IMG_H,IMG_W,19), generated_data,(count),MAP_SS)\n",
    "                count +=1\n",
    "                \n",
    "            for patch in range(self.args.no_of_patches_per_image):\n",
    "                \n",
    "#                 point_X_s = 256 * (patch +1)\n",
    "#                 point_Y_s = int(point_X_s/2)\n",
    "                \n",
    "#                 point_X = 512 - int(point_X_s/2)\n",
    "#                 point_Y = 256 - int(point_Y_s/2)\n",
    "                \n",
    "                if(self.args.patch_size[1] == 1024):\n",
    "                    X_Patch_resize = rl_img\n",
    "                    Y_Patch_resize = Y_pred\n",
    "                    map_patch_re = map_\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    point_X_s = self.args.patch_size[1]\n",
    "                    point_Y_s = self.args.patch_size[0]\n",
    "\n",
    "\n",
    "                    point_X = random.randint(0, IMG_W-point_X_s)\n",
    "                    point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "\n",
    "\n",
    "\n",
    "                    x_PATCH =  rl_img[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "                    y_PATCH =  Y_pred[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "                    map_patch = map_[point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s)\n",
    "                    \n",
    "                    \n",
    "                    ################resize on pytorch ###############\n",
    "                    x_PATCH = np.rollaxis(x_PATCH, axis = -1).reshape(1,3,point_Y_s,point_X_s)\n",
    "                    x_PATCH = torch.from_numpy(x_PATCH).float().to(device)\n",
    "                    \n",
    "                    y_PATCH = np.rollaxis(y_PATCH[0], axis = -1).reshape(1,19,point_Y_s,point_X_s)\n",
    "                    y_PATCH = torch.from_numpy(y_PATCH).float().to(device)\n",
    "                    \n",
    "                    map_patch = map_patch.reshape(1,1,point_Y_s,point_X_s)\n",
    "                    map_patch = torch.from_numpy(map_patch).float().to(device)\n",
    "                    \n",
    "                    X_Patch_resize = nn.functional.upsample(x_PATCH, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    X_Patch_resize = np.rollaxis(X_Patch_resize, axis = -1)\n",
    "                    X_Patch_resize = np.rollaxis(X_Patch_resize, axis = -1)\n",
    "                    \n",
    "                    \n",
    "                    Y_Patch_resize = nn.functional.upsample(y_PATCH, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1)\n",
    "                    Y_Patch_resize = np.rollaxis(Y_Patch_resize, axis = -1).reshape(1,IMG_H,IMG_W,19)\n",
    "                    \n",
    "                    \n",
    "                    map_patch_re = nn.functional.upsample(map_patch, (IMG_H, IMG_W), mode='bilinear', align_corners=True).cpu().data[0].numpy()\n",
    "                    map_patch_re = map_patch_re[0]\n",
    "                   \n",
    "                    #####################################\n",
    "                    \n",
    "                    \n",
    "#                     X_Patch_resize = resize(x_PATCH, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "#                     Y_Patch_resize = resize(y_PATCH, (1,IMG_H, IMG_W), anti_aliasing=False)\n",
    "#                     map_patch_re = resize(map_patch, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "                \n",
    "\n",
    "\n",
    "                f.write(str(count+patch)+\".png\"+'\\n')\n",
    "                self.save_data(X_Patch_resize, Y_Patch_resize.reshape(IMG_H,IMG_W,19), generated_data,(count+patch),map_patch_re)\n",
    "                \n",
    "                \n",
    "            count+=self.args.no_of_patches_per_image + 1\n",
    "        print(\"*****************END****************\")\n",
    "            \n",
    "             \n",
    "    \n",
    "                \n",
    "    def gaussian(self, ins,mean, stddev):\n",
    "  \n",
    "        noise = Variable(ins.data.new(ins.size()).normal_(mean,stddev))\n",
    "        \n",
    "        return ins + noise\n",
    "            \n",
    "    def train_adp(self,round_):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.args.data_label_folder_target = root_base +'/dataset/generated_data/'   \n",
    "        self.args.shuffel_ = True\n",
    "        self.args.data_dir_target = root_base +'/dataset/generated_data/'\n",
    "        self.args.data_list_target = data_gen_list\n",
    "        self.args.batch_size = 1\n",
    "        \n",
    "        _t = {'iter time' : Timer()}\n",
    "       \n",
    "        if(self.args.sorting_method == 'class_wise'):\n",
    "            self.args.num_steps = int(self.cnt_img * self.args.epoch_per_round * self.args.no_of_patches_per_image)\n",
    "        else:\n",
    "            \n",
    "            self.args.num_steps = int(self.args.p*self.args.total_no_of_target*self.args.no_of_patches_per_image *self.args.epoch_per_round)\n",
    "     \n",
    "            \n",
    "           \n",
    "        sourceloader, targetloader = CreateSrcDataLoader(self.args), CreateTrgDataLoader(self.args)\n",
    "        targetloader_iter, sourceloader_iter = iter(targetloader), iter(sourceloader)\n",
    "        \n",
    "        \n",
    "        start_iter = 0\n",
    "#         if self.args.restore_from is not None:\n",
    "#             start_iter = int(self.args.restore_from.rsplit('/', 1)[1].rsplit('_')[1])\n",
    "        \n",
    "#         train_writer = tensorboardX.SummaryWriter(os.path.join(self.args.snapshot_dir, \"logs\", model_name))\n",
    "    \n",
    "#         bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        cudnn.enabled = True\n",
    "        cudnn.benchmark = True\n",
    "        self.model.train()\n",
    "        self.model.cuda()\n",
    "\n",
    "        loss = ['loss_seg_src', 'loss_seg_trg']\n",
    "        _t['iter time'].tic()\n",
    "        for i in range(start_iter, self.args.num_steps):\n",
    "            \n",
    "            self.model.adjust_learning_rate(self.args, self.optimizer, i)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            src_img, src_lbl, _, _ = sourceloader_iter.next()\n",
    "            \n",
    "            src_img, src_lbl = Variable(src_img).cuda(), Variable(src_lbl.long()).cuda()\n",
    "            src_seg_score = self.model(src_img, lbl=src_lbl)       \n",
    "            loss_seg_src = self.model.loss\n",
    "    \n",
    "            loss_src = torch.mean(loss_seg_src)     \n",
    "            ##############################\n",
    "            loss_src.backward()\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "           \n",
    "            \n",
    "            trg_img, trg_lbl, _, _ = targetloader_iter.next()\n",
    "            trg_img, trg_lbl = Variable(trg_img).cuda(), Variable(trg_lbl.long()).cuda()\n",
    "#             trg_img = self.gaussian(trg_img,0, 0.02)\n",
    "            trg_seg_score = self.model(trg_img, lbl=trg_lbl) \n",
    " \n",
    "            ############################\n",
    "            loss_seg_trg = self.model.loss \n",
    "            \n",
    "            ##########Focal loss############\n",
    "            loss_trg_2 = torch.mean(loss_seg_trg)\n",
    "            if(self.args.focal_flag):\n",
    "                    #try to minimize segmentation loss\n",
    "\n",
    "\n",
    "                pt = torch.exp(-loss_seg_trg)\n",
    "                loss_trg =   loss_seg_trg  * (1-pt)**self.args.gamma\n",
    "                trg_fcl = torch.mean(loss_trg)\n",
    "            else:\n",
    "                trg_fcl =0\n",
    "            \n",
    "            loss_trg =  0.1 *trg_fcl  + loss_trg_2\n",
    "            \n",
    "            \n",
    "            ##############################\n",
    "#             over_all_loss =   loss_trg     \n",
    "  \n",
    "            loss_trg.backward()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            src_seg_score, trg_seg_score = src_seg_score.detach(), trg_seg_score.detach()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "#             for m in loss:\n",
    "#                 train_writer.add_scalar(m, eval(m), i+1)\n",
    "\n",
    "            if (i+1) % 500 == 0:\n",
    "                print ('taking snapshot ...')\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.args.snapshot_dir, '%s_' %(self.args.source+\"_to_\" +self.args.target )+\"_\"+ self.args.Expriment_name +str(round_) +'.pth' ))   \n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                _t['iter time'].toc(average=False)\n",
    "                print ('[it %d][src seg loss %.4f][lr %.4f][%.2fs]' % \\\n",
    "                        (i + 1, loss_src.data, self.optimizer.param_groups[0]['lr']*10000, _t['iter time'].diff))\n",
    "                if i + 1 > self.args.num_steps_stop:\n",
    "                    print ('finish training')\n",
    "                    break\n",
    "                _t['iter time'].tic()\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGjOa-s6QqIe"
   },
   "source": [
    "\n",
    "\n",
    "load init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgQm8tD6QqIf"
   },
   "outputs": [],
   "source": [
    "model_initl = model_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNjv7BshQqIq"
   },
   "source": [
    "Experiment's Settigns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60jAkqx8QqIr"
   },
   "outputs": [],
   "source": [
    "model_initl.args.focal_flag = True\n",
    "model_initl.args.shrink_image = False\n",
    "\n",
    "model_initl.args.Expriment_name = 'Exp_Final_gta5_with_focal'     # name of an experiment\n",
    "model_initl.args.p =0.1                # p is the portion for selecion of samples in target data ==> p*Total number of images\n",
    "model_initl.args.no_of_patches_per_image = 4 # number of patchs extarct from each image\n",
    "model_initl.args.gamma = 5                 # (1-prediction)**gamma  ==> focal loss\n",
    "model_initl.args.learning_rate = 1e-6 # learning rate\n",
    "# model_initl.args.entropy_th = 1.3            # Entropy map's threshold \n",
    "Rounds = int(1/model_initl.args.p)           # Total number of rounds \n",
    "model_initl.args.epoch_per_round = 2   # epoch each round\n",
    "\n",
    "model_initl.args.sorting_method = 'class_wise'   # 'global' or 'class_wise'\n",
    "model_initl.args.total_no_of_target = 2975         # Total number of target images\n",
    "\n",
    "\n",
    "model_initl.args.patch_select_method = 'random'           # 'smart'  or 'random'\n",
    "model_initl.args.total_patches_in_smart=None     # toatal number of patches in smart selection\n",
    "model_initl.args.entropy_diff_th = None            # we select the specific patch which have entropy diff greater than this threshold\n",
    "                                                          # seleection range  0.1  to 0.6\n",
    "\n",
    "model_initl.args.balance_on_priors = False          # balance the prediction with priors info of area pred*(1-area)\n",
    "model_initl.args.entropy_normalization = True      # entropy normalization or not\n",
    "model_initl.args.patch_size = (256,512)\n",
    "\n",
    "model_initl.args.ent_th_on_class = True        #Apply unique entropy threshold of each class seperately\n",
    "model_initl.args.ent_th_on_class_gain = 1    # gain on class based entropy threshold \n",
    "\n",
    "\n",
    "print_args(model_initl.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1759,
     "status": "ok",
     "timestamp": 1581505357557,
     "user": {
      "displayName": "Muhammad Naseer Subhani",
      "photoUrl": "",
      "userId": "06699857406068047110"
     },
     "user_tz": -300
    },
    "id": "-TwG53R2QqIt",
    "outputId": "626afd3c-8a8c-4416-e569-5358203dcc87",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "           Expriment_name: Exp_Final_gta5_with_focal     \n",
      "        balance_on_priors: False                         \n",
      "               batch_size: 1                             \n",
      "                 data_dir: /home/olektra_gpu/Desktop/olektra projects/ECCV/dataset/gta5\n",
      "          data_dir_target: /home/olektra_gpu/Desktop/olektra projects/ECCV/dataset/cityscapes\n",
      " data_label_folder_target: None                          \n",
      "                data_list: /home/olektra_gpu/Desktop/olektra projects/ECCV/dataset/gta5_list/train.txt\n",
      "         data_list_target: /home/olektra_gpu/Desktop/olektra projects/ECCV/dataset/cityscapes_list/train.txt\n",
      "          ent_th_on_class: True                          \n",
      "     ent_th_on_class_gain: 1                             \n",
      "          entropy_diff_th: None                          \n",
      "    entropy_normalization: True                          \n",
      "          epoch_per_round: 2                             \n",
      "               focal_flag: True                          \n",
      "                    gamma: 5                             \n",
      "             init_weights: None                          \n",
      "        lambda_adv_target: 0.001                         \n",
      "            learning_rate: 1e-06                         \n",
      "          learning_rate_D: 0.0001                        \n",
      "                    model: DeepLab                       \n",
      "                 momentum: 0.9                           \n",
      "  no_of_patches_per_image: 4                             \n",
      "              num_classes: 19                            \n",
      "                num_steps: 250000                        \n",
      "           num_steps_stop: 120000                        \n",
      "              num_workers: 2                             \n",
      "                        p: 0.1                           \n",
      "      patch_select_method: random                        \n",
      "               patch_size: (256, 512)                    \n",
      "                    power: 0.9                           \n",
      "               print_freq: 100                           \n",
      "             restore_from: /home/olektra_gpu/Desktop/olektra projects/ECCV/init_models/init/gta5_DeepLab__init\n",
      "          save_pred_every: 10000                         \n",
      "                      set: train                         \n",
      "             shrink_image: False                         \n",
      "                 shuffel_: False                         \n",
      "             snapshot_dir: /home/olektra_gpu/Desktop/olektra projects/ECCV/snapshots/\n",
      "           sorting_method: class_wise                    \n",
      "                   source: gta5                          \n",
      "                   target: cityscapes                    \n",
      "       total_no_of_target: 2975                          \n",
      "   total_patches_in_smart: None                          \n",
      "             weight_decay: 0.0005                        \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "model_initl.opt.print_options(model_initl.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sB9sKyEaQqIx"
   },
   "outputs": [],
   "source": [
    "Base = Base_Adaptation(model_initl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939416,
     "status": "error",
     "timestamp": 1581506949625,
     "user": {
      "displayName": "Muhammad Naseer Subhani",
      "photoUrl": "",
      "userId": "06699857406068047110"
     },
     "user_tz": -300
    },
    "id": "TE4Vcf-MQqI0",
    "outputId": "9e23211f-af13-46b2-da65-5edae2535e1a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Round  0  #####################\n",
      "*********** Finding most confident samples using class_wise  ***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/olektra_gpu/.local/lib/python3.5/site-packages/torch/nn/functional.py:2416: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "7it [00:05,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "# Base.args.p = 0.1\n",
    "first_di = 0\n",
    "record_entropy = []\n",
    "record_mIoU_ = []\n",
    "prev_entropy = 12345666555433232234234\n",
    "for round_ in range(Rounds):\n",
    "    print(\"############### Round \",round_,\" #####################\")\n",
    "    Base.sorting()\n",
    "    Base.Data_Generate()\n",
    "    Base.train_adp(round_)\n",
    "    testing_entr, mIoU_ = run_time_evaluation.main(Base.model)\n",
    "    record_entropy.append(np.mean(testing_entr))\n",
    "    record_mIoU_.append(mIoU_)\n",
    "    Base.args.p = Base.args.p+0.05\n",
    "    print(\"#################### Round END #######################\")\n",
    "    diff = prev_entropy - np.mean(testing_entr)\n",
    "\n",
    "    if(diff < 2):\n",
    "        break\n",
    "#     if( diff  <  1 and round_ > 0):\n",
    "#         break\n",
    "#     if(round_ == 1):\n",
    "#         first_di = diff\n",
    "#     if(first_di != 0 and diff > 0):\n",
    "#         Base.args.learning_rate = Base.args.learning_rate * (diff/first_di)\n",
    "    if(Base.args.p > 0.5):\n",
    "        break\n",
    "    clear_output()\n",
    "    prev_entropy = np.mean(testing_entr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(Base.args.Expriment_name+'entr_record.npy', np.array(record_entropy))\n",
    "np.save(Base.args.Expriment_name+'Iou_record.npy', np.array(record_mIoU_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[238.91925, 211.53044, 201.52953, 181.916, 171.4761, 170.83728]\n"
     ]
    }
   ],
   "source": [
    "print(record_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPMuy8oQQqI3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1Pdy0EpQqI7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwAnfUf9QqI-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVy9L3HOQqJB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGKILoFmQqJE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cd1CV0AGQqJH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IESPUiBsQqJJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Fahxt6z2QqJM"
   },
   "source": [
    "new flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Da7BK-QqJP"
   },
   "outputs": [],
   "source": [
    "run_time_evaluation.main(Base.model,source_priros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDKoJhJbQqJT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPkUmicrQqJW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8DjGNuWQqJZ"
   },
   "outputs": [],
   "source": [
    "sourceloader, targetloader = CreateSrcDataLoader(Base.args), CreateTrgDataLoader(Base.args)\n",
    "targetloader_iter, sourceloader_iter = iter(targetloader), iter(sourceloader)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GneykW4OQqJc"
   },
   "outputs": [],
   "source": [
    "src_img, src_lbl, _, _ = sourceloader_iter.next()\n",
    "src_img=src_img.cpu().data[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brEgYqxyQqJf"
   },
   "outputs": [],
   "source": [
    "plt.imshow(src_img.reshape(720,1280,3)/255,interpolation='nearest')\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g74RszJKQqJl"
   },
   "outputs": [],
   "source": [
    "trg_img, trg_lbl, _, _ = targetloader_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIu86pmEQqJq"
   },
   "outputs": [],
   "source": [
    "print(src_lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "JqRmUY1LQqJu"
   },
   "source": [
    "\n",
    "\n",
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ac1EbFwQqJw"
   },
   "outputs": [],
   "source": [
    "def visualize(temp, plot=True):\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,20):\n",
    "        r[temp==l]=label_colours[l,0]\n",
    "        g[temp==l]=label_colours[l,1]\n",
    "        b[temp==l]=label_colours[l,2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = (r/255.0)#[:,:,0]\n",
    "    rgb[:,:,1] = (g/255.0)#[:,:,1]\n",
    "    rgb[:,:,2] = (b/255.0)#[:,:,2]\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "    else:\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QyNYAEuQqJz"
   },
   "outputs": [],
   "source": [
    "road = (128, 64,128)\n",
    "side_walk = (232, 35,244)\n",
    "building = (70, 70, 70)\n",
    "wall = (102,102,156)\n",
    "fence = (190,153,153)\n",
    "pole = (153,153,153)\n",
    "trafic_lights = (250,170, 30)\n",
    "trafic_sign = (220,220,  0)\n",
    "vegitation = (107,142, 35)\n",
    "terrain = (152,251,152)\n",
    "sky = (70,130,180)\n",
    "person = (220, 20, 60)\n",
    "rider = (255,  0,  0)\n",
    "car = ( 0,  0,142)\n",
    "truck = (0,  0, 70)\n",
    "bus = ( 0, 60,100)\n",
    "train = ( 0, 80,100)\n",
    "motorcycle = (0,  0,230)\n",
    "bicycle = (119, 11, 32)\n",
    "else_ = ( 0,  0,  0)\n",
    "\n",
    "\n",
    "\n",
    "label_colours = np.array([road, side_walk, building, wall,fence,\n",
    "                          pole, trafic_lights , trafic_sign , vegitation,  terrain ,  sky,  person, rider , car , truck ,bus  , train , motorcycle, bicycle ,else_  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19AdY-iwQqJ6"
   },
   "source": [
    "Entropy threshold for filtering of each class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36QiSjggQqJ7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# commutes = pd.DataFrame({'Entropy Threshold':Base.entropy_th_class.reshape(19)})   #Base.entropy_th_class.reshape(19)s\n",
    "\n",
    "# # commutes.plot(grid=True, bins=20, rwidth=0.9,\n",
    "# #                    color='#607c8e')\n",
    "# # plt.title('Commute Times for 1,000 Commuters')\n",
    "# # plt.xlabel('Counts')\n",
    "# # plt.ylabel('Commute Time')\n",
    "# # plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# ax = commutes.plot(grid=True,\n",
    "#                    color='#607c8e' ,kind='bar', figsize=(10,4)) # bar plots are categorical\n",
    "# ax.xaxis.set_visible(False) # table heading and x-labels over-printing\n",
    "\n",
    "# plt.title('Entropy threshold of each class')\n",
    "# plt.xlabel('Counts')\n",
    "# plt.ylabel('Threshold')\n",
    "\n",
    "# # add the ticks and labels to the plot\n",
    "# ax.set_xticks(xticks)\n",
    "\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAaGQB0gQqJ9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(Base.entropy_th_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuK6El7bQqKB"
   },
   "outputs": [],
   "source": [
    "class Analysis():\n",
    "    def __init__(self,model_loader):\n",
    "        self.args =model_loader.args\n",
    "        self.model_name =model_loader.model_name\n",
    "        self.model = model_loader.model\n",
    "        self.optimizer = model_loader.optimizer\n",
    "    def Show(self, rl_img,Y_pred,X_Patch_resize,Y_Patch_resize,Y_Patch_resize_div):\n",
    "        print(\"********Input Image*************\")\n",
    "        Plot_img(rl_img)\n",
    "\n",
    "        print(\"********Prediction*************\")\n",
    "        a =np.argmax(Y_pred.T,axis=0)\n",
    "        true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "        Plot_img(true_lab)\n",
    "        \n",
    "        print(\"********Patch*************\")\n",
    "        Plot_img(X_Patch_resize)\n",
    "        \n",
    "        print(\"********Patch Pseudo labels*************\")\n",
    "        a =np.argmax(Y_Patch_resize.T,axis=0)\n",
    "        true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "        Plot_img(true_lab)\n",
    "        \n",
    "        print(\"********Patch Pseudo labels with division*************\")\n",
    "        a =np.argmax(Y_Patch_resize_div.T,axis=0)\n",
    "        true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "        Plot_img(true_lab)\n",
    "        \n",
    "    def Final_image_results(self, num_of_samples):\n",
    "        self.args.data_label_folder_target = None\n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        print(\"***********Results output***********\")\n",
    "        \n",
    "        for index, batch in enumerate(targetloader):\n",
    "            print(\"###################################\")\n",
    "            if(index > num_of_samples):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            Y_p = keras_out(output)    # later delete this line man\n",
    "\n",
    "                    \n",
    "            \n",
    "            \n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            ################################################\n",
    "#             map_ = np.zeros((IMG_H,IMG_W))\n",
    "#             SE = self_entropy(Y_pred)\n",
    "            \n",
    "#             map_[SE >= entropy_th] =  0\n",
    "#             map_[SE < entropy_th] =  1\n",
    "            \n",
    "            ######################\n",
    "            print(name)\n",
    "            print(\"********Target image*************\")\n",
    "            Plot_img(rl_img)\n",
    "            print(\"********whole Prediction*************\")\n",
    "            a =np.argmax(Y_pred.T,axis=0)\n",
    "            true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "            Plot_img(true_lab)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    def Pseudo_labels_analysis(self, num_of_samples):\n",
    "        self.args.data_label_folder_target = None\n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        print(\"***********Testing Pseudo labels***********\")\n",
    "        \n",
    "        for index, batch in enumerate(targetloader):\n",
    "            print(\"###################################\")\n",
    "            if(index > num_of_samples):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            Y_p = keras_out(output)    # later delete this line man\n",
    "            if(balance_):\n",
    "                wei = (1 - weight_mat )** d1\n",
    "                output = np.multiply(output,wei)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            ################################################\n",
    "            map_ = np.zeros((IMG_H,IMG_W))\n",
    "            SE = self_entropy(Y_pred)\n",
    "            \n",
    "            map_[SE >= entropy_th] =  0\n",
    "            map_[SE < entropy_th] =  1\n",
    "            \n",
    "            ######################\n",
    "            print(\"********Target image*************\")\n",
    "            Plot_img(rl_img)\n",
    "            print(\"********whole Prediction*************\")\n",
    "            a =np.argmax(Y_pred.T,axis=0)\n",
    "            true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "            Plot_img(true_lab)\n",
    "            print(\"********whole Filtering map*************\")\n",
    "            plt.imshow(map_.reshape(IMG_H,IMG_W),cmap='gray')\n",
    "            plt.grid(False)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for patch in range(patches):\n",
    "                \n",
    "\n",
    "                point_X_s = 512\n",
    "                point_Y_s = 256\n",
    "                \n",
    "                point_X = random.randint(0, IMG_W-point_X_s)\n",
    "                point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "\n",
    "            \n",
    "                x_PATCH =  rl_img[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "                y_PATCH =  Y_pred[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "                map_patch = map_[point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s)\n",
    "   \n",
    "                X_Patch_resize = resize(x_PATCH, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "                Y_Patch_resize = resize(y_PATCH, (1,IMG_H, IMG_W), anti_aliasing=False)\n",
    "                map_patch_re = resize(map_patch, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "        \n",
    "                \n",
    "\n",
    "                #f.write(str(count+patch)+\".png\"+'\\n')\n",
    "                #self.save_data(X_Patch_resize, Y_Patch_resize.reshape(IMG_H,IMG_W,19), generated_data,(count+patch),map_)\n",
    "                \n",
    "            #count+=patches + 1\n",
    "        #print(\"*****************END****************\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ########################################################\n",
    "                print(\"********patch*************\")\n",
    "                Plot_img(X_Patch_resize)\n",
    "\n",
    "                print(\"******** patch Prediction*************\")\n",
    "                a =np.argmax(Y_Patch_resize.T,axis=0)\n",
    "                true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "                Plot_img(true_lab)\n",
    "            \n",
    "#             print(\"********Prediction after balance*************\")\n",
    "#             a =np.argmax(Y_pred.T,axis=0)\n",
    "#             true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "#             Plot_img(true_lab)\n",
    "            \n",
    "#             print(\"********Self entropy*************\")\n",
    "#             plt.imshow(SE.reshape(IMG_H,IMG_W),interpolation='nearest')\n",
    "#             plt.grid(False)\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "            \n",
    "                print(\"******** patch Filtering map*************\")\n",
    "                plt.imshow(map_patch_re.reshape(IMG_H,IMG_W),cmap='gray')\n",
    "                plt.grid(False)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             h = random.randint(16, 32)\n",
    "#             point_X_s = int(32 * h)\n",
    "#             point_Y_s = int(point_X_s/2)\n",
    "\n",
    "#             point_X = random.randint(0, IMG_W-point_X_s)\n",
    "#             point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "\n",
    "\n",
    "#             x_PATCH =  rl_img[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "#             y_PATCH =  Y_pred[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "\n",
    "#             X_Patch_resize = resize(x_PATCH, (IMG_H, IMG_W), anti_aliasing=True)\n",
    "#             Y_Patch_resize = resize(y_PATCH, (1,IMG_H, IMG_W), anti_aliasing=True)\n",
    "            \n",
    "            \n",
    "# #             map_1 = weights + epsilon   \n",
    "# #             Y_pred_divide = np.divide(Y_pred,map_1)\n",
    "#             output = np.divide(output,blurred)  \n",
    "#             Y_pred_divide = keras_out(output)\n",
    "            \n",
    "#             y_PATCH_div =  Y_pred_divide[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "#             Y_Patch_resize_div = resize(y_PATCH_div, (1,IMG_H, IMG_W), anti_aliasing=True)\n",
    "            \n",
    "\n",
    "#             self.Show(rl_img,Y_pred,X_Patch_resize,Y_Patch_resize,Y_Patch_resize_div)\n",
    "        \n",
    "            print(\"###################################\")\n",
    "            \n",
    "    def Pseudo_labels_analysis_2(self, num_of_samples):\n",
    "        self.args.data_label_folder_target = None\n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        print(\"***********Testing Pseudo labels***********\")\n",
    "        \n",
    "        for index, batch in enumerate(targetloader):\n",
    "            print(\"###################################\")\n",
    "            if(index > num_of_samples):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            Y_p = keras_out(output)    # later delete this line man\n",
    "            \n",
    "            image=image.cpu().data[0].numpy()\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            ################################################\n",
    "            SE = self_entropy(Y_pred)\n",
    "            ########################################################\n",
    "            Plot_img(rl_img)\n",
    "          \n",
    "            print(\"******** full image Prediction*************\")\n",
    "            a =np.argmax(Y_pred.T,axis=0)\n",
    "            true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "            Plot_img(true_lab)\n",
    "            \n",
    "            print(\"********Self entropy*************\")\n",
    "            plt.imshow(SE.reshape(IMG_H,IMG_W),interpolation='nearest')\n",
    "            plt.grid(False)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            #################################################\n",
    "            rl_patch = []\n",
    "            se_Main = []\n",
    "            se_Patch = []\n",
    "            se_diff=[]\n",
    "            for patch in range(patches):\n",
    "                \n",
    "\n",
    "                point_X_s = 512\n",
    "                point_Y_s = 256\n",
    "                \n",
    "                point_X = random.randint(0, IMG_W-point_X_s)\n",
    "                point_Y = random.randint(0, IMG_H - point_Y_s)\n",
    "\n",
    "                real_patch =  rl_img[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "               \n",
    "                x_PATCH =  image[ point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(point_Y_s,point_X_s,3)\n",
    "                y_PATCH =  Y_pred[0, point_Y:point_Y+point_Y_s, point_X:point_X+point_X_s].reshape(1,point_Y_s,point_X_s,19)\n",
    "               \n",
    "                X_Patch_resize = resize(x_PATCH, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "                Y_Patch_resize = resize(y_PATCH, (1,IMG_H, IMG_W), anti_aliasing=False)\n",
    "                \n",
    "                real_patch_resize = resize(real_patch, (IMG_H, IMG_W), anti_aliasing=False)\n",
    "                \n",
    "                X_Patch_resize = np.rollaxis(X_Patch_resize, axis = -1).reshape(1,3,IMG_H,IMG_W)\n",
    "                X_Patch_resize = torch.from_numpy(X_Patch_resize).float().to(device)\n",
    "              \n",
    "                \n",
    "                output = self.model(Variable(X_Patch_resize).cuda())\n",
    "                output = nn.functional.softmax(output, dim=1)\n",
    "                output=output.cpu().data[0].numpy()\n",
    "                y_patch_out = keras_out(output)\n",
    "                \n",
    "                SE_main = self_entropy(Y_Patch_resize)\n",
    "                SE_patch = self_entropy(y_patch_out)\n",
    "                \n",
    "                diff = (np.mean(SE_patch) - np.mean(SE_main))\n",
    "                \n",
    "                \n",
    "                \n",
    "                rl_patch.append(real_patch_resize)\n",
    "                se_Main.append(SE_main)\n",
    "                se_Patch.append(SE_patch)\n",
    "                se_diff.append(diff)\n",
    "                \n",
    "            se_diff = np.array(se_diff)\n",
    "            arg_max =np.argmax(se_diff)\n",
    "            maxi = np.max(se_diff)\n",
    "            print(maxi)\n",
    "            \n",
    "            Plot_img(rl_patch[arg_max])\n",
    "#             print(\"********Self entropy real*************\")\n",
    "#             plt.imshow(se_Main[arg_max].reshape(IMG_H,IMG_W),interpolation='nearest')\n",
    "#             plt.grid(False)\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "\n",
    "            print(\"******** patch Prediction*************\")\n",
    "            a =np.argmax(y_patch_out.T,axis=0)\n",
    "            true_lab = visualize(a.T.reshape((IMG_H,IMG_W)), False)\n",
    "            Plot_img(true_lab)\n",
    "            \n",
    "            print(\"********Self entropy patch*************\")\n",
    "            plt.imshow(se_Patch[arg_max].reshape(IMG_H,IMG_W),interpolation='nearest')\n",
    "            plt.grid(False)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            print(\"######################################################\")\n",
    "        \n",
    "           \n",
    "                \n",
    "\n",
    "            \n",
    "    def Analyze_class_frequency(self,num_of_samples ):\n",
    "        self.args.data_label_folder_target = None\n",
    "        self.args.shuffel_ = False\n",
    "        self.args.data_dir_target = root_base +'/dataset/cityscapes'\n",
    "        self.args.data_list_target = sorted_list\n",
    "        self.model.eval()\n",
    "        self.model.cuda()    \n",
    "        \n",
    "        targetloader = CreateTrgDataLoader(self.args)\n",
    "        \n",
    "        print(\"***********Testing Pseudo labels***********\")\n",
    "        \n",
    "        class_freq = 0\n",
    "        \n",
    "        for index, batch in enumerate(targetloader):\n",
    "            \n",
    "            if(index > num_of_samples):\n",
    "                break\n",
    "            \n",
    "            image, _, name, rl_img = batch\n",
    "            output = self.model(Variable(image).cuda())\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            rl_img = rl_img.cpu().data[0].numpy()/255.0\n",
    "            output=output.cpu().data[0].numpy()\n",
    "            Y_p = keras_out(output)    # later delete this line man\n",
    "            \n",
    "            image=image.cpu().data[0].numpy()\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            image = np.rollaxis(image, axis = -1)\n",
    "            Y_pred = keras_out(output)\n",
    "            \n",
    "            class_are =   np.mean(Y_pred,axis = (0,1,2))\n",
    "            \n",
    "            class_freq += class_are\n",
    "        \n",
    "        return class_freq/num_of_samples\n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvBB1GsqQqKE"
   },
   "outputs": [],
   "source": [
    "# Base.sorting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4QnTVbtQqKJ"
   },
   "outputs": [],
   "source": [
    "entropy_th = 0.4\n",
    "patches = 5\n",
    "balance_  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AaWz3I2BQqKN"
   },
   "outputs": [],
   "source": [
    "ana =Analysis(model_initl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PpggcLTQqKQ"
   },
   "outputs": [],
   "source": [
    "ana.Final_image_results( 82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chtf0iO7QqKT"
   },
   "source": [
    "block diagram image select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCUmQsTzQqKU"
   },
   "outputs": [],
   "source": [
    "ana.Pseudo_labels_analysis(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vufI0XDDQqKa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NxRJwBUQqKd"
   },
   "outputs": [],
   "source": [
    "ana.Pseudo_labels_analysis_2(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llz_raCUQqKf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ana.Pseudo_labels_analysis_2(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-knp3uPCQqKg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_=ana.Analyze_class_frequency(100\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVWc7eTCQqKi"
   },
   "outputs": [],
   "source": [
    "class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lv4G7RKjQqKk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtMFQPQPQqKp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fI-1NP9gQqKs"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWrWdhbWQqKv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXugA_9LQqKw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssRR-f-nQqK0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vZPwpOhQqK6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHxRhLRRQqK7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0u9REzxQqK9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHEUw9WJQqLA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqGN310DQqLH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzRlkU9ZQqLK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exp_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
